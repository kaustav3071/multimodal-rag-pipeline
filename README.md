# ğŸš€ Multimodal RAG Pipeline

> **Advanced Retrieval-Augmented Generation (RAG) system with intelligent chunking, vector embeddings, and semantic search capabilities**

[![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)](https://www.python.org/)
[![LangChain](https://img.shields.io/badge/LangChain-Latest-green.svg)](https://www.langchain.com/)
[![ChromaDB](https://img.shields.io/badge/ChromaDB-Vector%20DB-orange.svg)](https://www.trychroma.com/)

---

## ğŸ“‹ Table of Contents

- [Overview](#-overview)
- [Architecture](#-architecture)
- [Features](#-features)
- [Project Structure](#-project-structure)
- [Installation](#-installation)
- [Usage](#-usage)
- [Chunking Methods](#-chunking-methods)
- [Pipeline Components](#-pipeline-components)
- [Configuration](#-configuration)
- [Contributing](#-contributing)
- [License](#-license)

---

## ğŸ¯ Overview

This **Multimodal RAG Pipeline** is a sophisticated document retrieval and question-answering system that combines:

- ğŸ§  **Advanced Chunking Strategies** - Multiple text splitting methods for optimal context preservation
- ğŸ” **Semantic Search** - Vector embeddings using HuggingFace Transformers
- ğŸ’¾ **Persistent Vector Storage** - ChromaDB for efficient similarity search
- ğŸ¤– **LLM Integration** - OpenAI GPT models for intelligent answer generation
- ğŸ“š **Multi-Document Support** - Process and query across multiple knowledge sources
- âš¡ **History-Aware Generation** - Context-aware conversational retrieval

---

## ğŸ—ï¸ Architecture

### 1ï¸âƒ£ Knowledge Base Construction (Ingestion Pipeline)

The ingestion pipeline processes source documents through chunking, embedding, and storage:

![Ingestion Pipeline Architecture](images/architecture_ingestion.png)

**Pipeline Flow:**
1. **Source Documents** (~10M tokens) â†’ Raw text files from various sources
2. **Chunking** (~1K tokens) â†’ Intelligent text splitting with configurable chunk sizes
3. **Embedding** â†’ Convert chunks into 2000-dimensional vector embeddings
4. **Vector DB** â†’ Store embeddings in ChromaDB with cosine similarity indexing

---

### 2ï¸âƒ£ Retrieval Pipeline

The retrieval pipeline handles user queries and generates contextual answers:

![Retrieval Pipeline Architecture](images/architecture_retrieval.png)

**Pipeline Flow:**
1. **User Query** â†’ Natural language question input
2. **Query Embedding** â†’ Convert query to vector representation
3. **Similarity Search** â†’ Retrieve top-K most relevant chunks from Vector DB
4. **Context Assembly** â†’ Combine retrieved chunks with query
5. **LLM Generation** â†’ Generate accurate, context-aware answers

---

## âœ¨ Features

| Feature | Description |
|---------|-------------|
| ğŸ”„ **Multiple Chunking Methods** | Character-based, Semantic, and Agentic chunking strategies |
| ğŸ¯ **Semantic Search** | Sentence-Transformers for high-quality embeddings |
| ï¿½ **Hybrid Search** | Combines vector (semantic) + BM25 (keyword) for superior accuracy |
| ğŸ§  **Reranker (Cross-Encoder)** | Two-stage retrieval with Cohere reranking for precision |
| ï¿½ğŸ’¬ **Conversational RAG** | History-aware generation for multi-turn conversations |
| ğŸ“Š **Rich Document Support** | Process `.txt`, `.pdf`, and structured documents |
| âš™ï¸ **Configurable Pipeline** | Customizable chunk sizes, overlap, and retrieval parameters |
| ğŸ” **Environment Management** | Secure API key handling with `.env` configuration |
| ğŸš€ **Production Ready** | Persistent storage, error handling, and logging |

---

## ğŸ“‚ Project Structure

```
RAG/
â”œâ”€â”€ ğŸ“ RAG pipeline/
â”‚   â”œâ”€â”€ ingestion_pipeline.py      # Document loading, chunking, and embedding
â”‚   â”œâ”€â”€ retrieval_pipeline.py      # Query processing and document retrieval
â”‚   â”œâ”€â”€ answer_generation.py       # LLM-based answer generation
â”‚   â””â”€â”€ history_aware_generation.py # Conversational RAG with context
â”‚
â”œâ”€â”€ ğŸ“ Chunking Method/
â”‚   â”œâ”€â”€ character_text_splitter.py # Basic character-based chunking
â”‚   â”œâ”€â”€ sematic_text_splitter.py   # Semantic chunking (meaning preservation)
â”‚   â””â”€â”€ agentic_chunking.py        # AI-powered intelligent chunking
â”‚
â”œâ”€â”€ ğŸ“„ all_retrieval_method.py      # Compare similarity, threshold & MMR retrieval
â”œâ”€â”€ ğŸ“„ multi_query_retrieval.py     # Multi-query expansion with LLM
â”œâ”€â”€ ğŸ“„ reciprocal_rank_fusion.py    # RRF for combining multi-query results
â”‚
â”œâ”€â”€ ï¿½ hybrid_search.ipynb          # Hybrid search demo (Vector + BM25)
â”œâ”€â”€ ğŸ““ reranker.ipynb               # Reranker cross-encoder demo (Cohere)
â”‚
â”œâ”€â”€ ï¿½ğŸ“ images/                      # Visual documentation and diagrams
â”‚   â”œâ”€â”€ rrf_simple_explanation.png # RRF concept visualization
â”‚   â”œâ”€â”€ rrf_k60_example.png        # RRF calculation with k=60
â”‚   â”œâ”€â”€ rrf_key_takeaways.png      # RRF benefits summary
â”‚   â”œâ”€â”€ hybrid_search_flow.png     # Hybrid search architecture
â”‚   â”œâ”€â”€ reranker_cross_encoder.png # Cross-encoder mechanism
â”‚   â”œâ”€â”€ reranker_two_stage_strategy.png # Two-stage retrieval strategy
â”‚   â”œâ”€â”€ reranker_why_needed.png    # Why rerankers are necessary
â”‚   â””â”€â”€ reranker_comparison.png    # Before/after reranking comparison
â”‚
â”œâ”€â”€ ğŸ“ docs/
â”‚   â”œâ”€â”€ google.txt                 # Google company information
â”‚   â”œâ”€â”€ microsoft.txt              # Microsoft company information
â”‚   â”œâ”€â”€ nvidia.txt                 # NVIDIA company information
â”‚   â”œâ”€â”€ spacex.txt                 # SpaceX company information
â”‚   â”œâ”€â”€ tesla.txt                  # Tesla company information
â”‚   â””â”€â”€ paper.pdf                  # Research paper (multimodal)
â”‚
â”œâ”€â”€ ğŸ“ database/                   # Cached embeddings and vector stores
â”œâ”€â”€ ğŸ“ db/                         # ChromaDB persistent storage
â”‚
â”œâ”€â”€ ğŸ““ multi_modal_rag.ipynb       # Interactive Jupyter notebook demo
â”œâ”€â”€ ğŸ“„ requirements.txt            # Python dependencies
â”œâ”€â”€ ğŸ”’ .env                        # Environment variables (API keys)
â”œâ”€â”€ ğŸš« .gitignore                  # Git ignore configuration
â””â”€â”€ ğŸ“– README.md                   # This file
```

---

## ğŸ› ï¸ Installation

### Prerequisites

- **Python 3.11+** ğŸ
- **pip** package manager
- **OpenAI API Key** (for LLM generation)

### Step 1: Clone the Repository

```bash
git clone https://github.com/yourusername/multimodal-rag-pipeline.git
cd multimodal-rag-pipeline
```

### Step 2: Create Virtual Environment

```bash
# Create virtual environment
python -m venv .venv

# Activate virtual environment
# Windows (PowerShell)
.venv\Scripts\Activate.ps1

# Linux/Mac
source .venv/bin/activate
```

### Step 3: Install Dependencies

```bash
pip install -r requirements.txt
```

### Step 4: Configure Environment Variables

Create a `.env` file in the project root:

```env
# OpenAI Configuration
OPENAI_API_KEY=your_openai_api_key_here

# OpenRouter Configuration (for Multi-Query Retrieval)
OPENROUTER_API_KEY=your_openrouter_api_key_here

# Cohere Configuration (for Reranking)
COHERE_API_KEY=your_cohere_api_key_here

# Optional: Model Configuration
OPENAI_MODEL=gpt-4-turbo-preview
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2

# Optional: Database Configuration
CHROMA_PERSIST_DIR=db/chroma_db
```

---

## ğŸš€ Usage

### 1ï¸âƒ£ Build Knowledge Base (Ingestion)

Process documents and create vector embeddings:

```bash
python "RAG pipeline/ingestion_pipeline.py"
```

**This will:**
- âœ… Load all documents from the `docs/` folder
- âœ… Split documents into optimized chunks
- âœ… Generate vector embeddings using HuggingFace models
- âœ… Store embeddings in ChromaDB at `db/chroma_db/`

### 2ï¸âƒ£ Query Documents (Retrieval)

Search for relevant information:

```bash
python "RAG pipeline/retrieval_pipeline.py"
```

**Example Query:**
```python
query = "What was NVIDIA's first graphics accelerator called?"
```

**Output:**
```
User Query: What was NVIDIA's first graphics accelerator called?
--------------------------------------------------
Document 1:
NVIDIA's first product was the NV1, released in 1995...

Document 2:
The NV1 was a multimedia graphics accelerator...
```

### 3ï¸âƒ£ Generate Answers (RAG)

Get AI-generated answers from your documents:

```bash
python "RAG pipeline/answer_generation.py"
```

**Features:**
- ğŸ¤– Uses retrieved context to generate accurate answers
- ğŸ“ Cites source documents
- âš¡ Handles multi-document queries

### 5ï¸âƒ£ Advanced Retrieval Methods

Compare different retrieval strategies:

```bash
python all_retrieval_method.py
```

**This will demonstrate:**
- âœ… Basic similarity search (top-K retrieval)
- âœ… Score threshold filtering (quality control)
- âœ… Maximum Marginal Relevance (diversity optimization)

**Example Output:**
```
=== METHOD 1: Basic similarity search ===
Retrieved 3 documents

=== METHOD 2: Similarity with Score Threshold ===
Retrieved 2 documents (threshold: 0.45)

=== METHOD 3: Maximum Marginal Relevance (MMR) ===
Retrieved 3 documents (Î»=0.5)
```

### 6ï¸âƒ£ Multi-Query Retrieval

Use LLM-powered query expansion for comprehensive search:

```bash
python multi_query_retrieval.py
```

**Features:**
- ğŸ¤– Automatically generates 3 query variations using GPT-4o-mini
- ğŸ” Searches with each variation for broader coverage
- ğŸ“Š Aggregates results from multiple perspectives

**Example:**
```
Original Query: How does Tesla make money?

Generated Query Variations:
1. What are Tesla's primary revenue streams?
2. How does Tesla generate income and profits?
3. What business models does Tesla use to earn revenue?

=== RESULTS FOR QUERY 1 ===
Retrieved 5 documents...
```

### 4ï¸âƒ£ Reciprocal Rank Fusion

Combine multiple query results using advanced RRF scoring:

```bash
python reciprocal_rank_fusion.py
```

**Features:**
- ğŸ¤– Generates query variations using GPT-4o-mini
- ğŸ” Retrieves documents for each variation
- ğŸ“Š Applies RRF algorithm to create consensus ranking
- ğŸ¯ Boosts documents appearing in multiple results

**Example Output:**
```
Original Query: How does Tesla make money?

Generated Query Variations:
1. What are Tesla's primary revenue streams?
2. How does Tesla generate income and profits?
3. What business models does Tesla use to earn revenue?

=== Reciprocal Rank Fusion ===
Using k=60 for RRF calculation

Processing Query 1 results:
Chunk_1 (position 1): 0.0164
Chunk_2 (position 2): 0.0161
...

=== Fused Results ===
Final ranking based on consensus across all queries
```


### 7ï¸âƒ£ Conversational RAG

Interactive question-answering with conversation history:

```bash
python "RAG pipeline/history_aware_generation.py"
```

**Features:**
- ğŸ’¬ Multi-turn conversations
- ğŸ§  Context-aware follow-up questions
- ğŸ”„ Automatic conversation history management

### 8ï¸âƒ£ Jupyter Notebook Demos

Explore the interactive demos:

**Multimodal RAG:**

```bash
jupyter notebook multi_modal_rag.ipynb
```

**Hybrid Search (Vector + BM25):**

```bash
jupyter notebook hybrid_search.ipynb
```

**Features:**
- ğŸ” Compare vector-only vs BM25-only vs hybrid retrieval
- ğŸ“Š See real-time performance differences
- ğŸ¯ Test with various query types (semantic, keyword, mixed)

**Reranker (Cross-Encoder):**

```bash
jupyter notebook reranker.ipynb
```

**Features:**
- ğŸ§  Demonstrates two-stage retrieval strategy
- ğŸ“ˆ Shows before/after reranking comparison
- âœ¨ Uses Cohere's rerank-english-v3.0 model

---

## ğŸ§© Chunking Methods

The project implements three sophisticated chunking strategies:

### 1. Character Text Splitter ğŸ“

**File:** `Chunking Method/character_text_splitter.py`

**Strategy:** Fixed-size character-based chunking with overlap

```python
chunk_size = 800 characters
chunk_overlap = 0 characters
```

**Best For:**
- âœ… General text documents
- âœ… Uniform content structure
- âœ… Quick prototyping

---

### 2. Semantic Text Splitter ğŸ§ 

**File:** `Chunking Method/sematic_text_splitter.py`

**Strategy:** Meaning-preserving chunks based on semantic boundaries

```python
- Preserves sentence structure
- Maintains context coherence
- Respects document sections
```

**Best For:**
- âœ… Complex documents
- âœ… Technical documentation
- âœ… Narrative text

---

### 3. Agentic Chunking ğŸ¤–

**File:** `Chunking Method/agentic_chunking.py`

**Strategy:** AI-powered intelligent chunking using LLM analysis

```python
- Identifies logical sections
- Preserves topic coherence
- Adapts to content type
```

**Best For:**
- âœ… Mixed content types
- âœ… Research papers
- âœ… Maximum accuracy

---

## âš™ï¸ Pipeline Components

### ğŸ”µ Ingestion Pipeline

**Purpose:** Transform raw documents into searchable vector embeddings

**Key Functions:**

```python
load_documents(docs_path="docs")              # Load all text files
split_documents(documents, chunk_size=800)    # Chunk documents
create_vector_store(chunks, persist_dir)      # Create ChromaDB
```

**Technologies:**
- `LangChain` - Document processing framework
- `HuggingFace Embeddings` - Sentence transformers
- `ChromaDB` - Vector database with HNSW indexing

---

### ğŸŸ¢ Retrieval Pipeline

**Purpose:** Find relevant document chunks for user queries

**Key Features:**

```python
# Initialize retriever
retriever = db.as_retriever(search_kwargs={"k": 5})

# Retrieve top-5 relevant chunks
relevant_docs = retriever.invoke(query)
```

**Similarity Metric:** Cosine similarity in embedding space

---

### ğŸŸ¡ Answer Generation

**Purpose:** Generate accurate answers using retrieved context

**Technologies:**
- `OpenAI GPT-4` - Language model
- `LangChain` - Prompt engineering and chains
- `Context Injection` - Retrieval-augmented prompts

**Example Prompt Template:**

```python
system_message = """
You are an expert assistant. Use the following context 
to answer the user's question accurately.

Context: {context}
"""
```

---

### ğŸŸ£ History-Aware Generation

**Purpose:** Multi-turn conversational RAG with context memory

**Features:**
- ğŸ’¾ Conversation history tracking
- ğŸ”„ Context-aware query reformulation
- ğŸ“ Session management

---

## ğŸ“Š Configuration

### Embeddings Configuration

```python
embedding_model = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)
```

**Embedding Dimensions:** 384 (configurable)

### Vector Store Configuration

```python
vectorstore = Chroma.from_documents(
    documents=chunks,
    embedding=embedding_model,
    persist_directory="db/chroma_db",
    collection_metadata={"hnsw:space": "cosine"}
)
```

**Index Type:** HNSW (Hierarchical Navigable Small World)
**Similarity Metric:** Cosine similarity

### Retrieval Configuration

```python
retriever = db.as_retriever(
    search_type="similarity",
    search_kwargs={
        "k": 5,              # Number of documents to retrieve
        "score_threshold": 0.7  # Minimum similarity score
    }
)
```

---

## ğŸ” Advanced Retrieval Methods

The pipeline supports multiple sophisticated retrieval strategies to optimize document search quality and relevance:

### Method 1: Basic Similarity Search

**File:** `all_retrieval_method.py`

The standard vector similarity search retrieves the top-K most similar documents based on cosine similarity:

```python
retriever = db.as_retriever(search_kwargs={"k": 3})
docs = retriever.invoke(query)
```

**Best For:**
- âœ… General purpose queries
- âœ… Fast retrieval needs
- âœ… Balanced relevance

---

### Method 2: Similarity with Score Threshold

**File:** `all_retrieval_method.py`

Filters results to only return documents exceeding a minimum similarity threshold, ensuring high-quality matches:

```python
retriever = db.as_retriever(
    search_type="similarity_score_threshold",
    search_kwargs={
        "k": 3,
        "score_threshold": 0.45  # Only return docs with similarity >= 0.45
    }
)
```

**Best For:**
- âœ… High-precision requirements
- âœ… Filtering low-quality matches
- âœ… Strict relevance criteria

**Parameters:**
- `k`: Maximum number of documents to retrieve
- `score_threshold`: Minimum similarity score (0.0 to 1.0)

---

### Method 3: Maximum Marginal Relevance (MMR)

**File:** `all_retrieval_method.py`

Balances relevance and diversity to avoid redundant results by selecting documents that are both relevant to the query and diverse from each other:

```python
retriever = db.as_retriever(
    search_type="mmr",
    search_kwargs={
        "k": 3,           # Final number of docs
        "fetch_k": 10,    # Initial pool to select from
        "lambda_mult": 0.5  # 0=max diversity, 1=max relevance
    }
)
```

**Best For:**
- âœ… Avoiding redundant information
- âœ… Comprehensive topic coverage
- âœ… Diverse perspectives

**Parameters:**
- `k`: Number of documents to return
- `fetch_k`: Initial candidate pool size
- `lambda_mult`: Balance factor
  - `0.0` = Maximum diversity
  - `1.0` = Maximum relevance
  - `0.5` = Balanced approach

---

### Method 4: Multi-Query Retrieval

**File:** `multi_query_retrieval.py`

Advanced technique that generates multiple query variations using an LLM to improve retrieval coverage and handle complex questions:

```python
# Step 1: Generate query variations using LLM
original_query = "How does Tesla make money?"
llm_with_tools = llm.with_structured_output(QueryVariations)
query_variations = llm_with_tools.invoke(prompt).queries

# Step 2: Search with each variation
retriever = db.as_retriever(search_kwargs={"k": 5})
for query in query_variations:
    docs = retriever.invoke(query)
    all_retrieval_results.append(docs)
```

**Best For:**
- âœ… Complex, multi-faceted queries
- âœ… Ambiguous questions
- âœ… Comprehensive information gathering
- âœ… Handling varied phrasings

**How It Works:**
1. **Query Expansion:** LLM generates 3 alternative phrasings of the original query
2. **Parallel Retrieval:** Each variation searches the vector database independently
3. **Result Aggregation:** Combines results for comprehensive coverage

**Technologies:**
- `OpenAI GPT-4o-mini` via OpenRouter - Query variation generation
- `Pydantic` - Structured output validation
- `ChromaDB` - Multi-query vector search

**Example Query Variations:**

Original: *"How does Tesla make money?"*

Generated Variations:
1. "What are Tesla's primary revenue streams?"
2. "How does Tesla generate income and profits?"
3. "What business models does Tesla use to earn revenue?"

---

### Method 5: Reciprocal Rank Fusion (RRF)

**File:** `reciprocal_rank_fusion.py`

Reciprocal Rank Fusion is an advanced ensemble technique that combines multiple retrieval results by aggregating ranking positions. It's particularly powerful when used with multi-query retrieval to create a "consensus" ranking that boosts documents appearing in multiple result sets.

#### ğŸ¯ How RRF Works

![RRF Simple Explanation](images/rrf_simple_explanation.png)

**The RRF Formula:**

```
RRF_score = Î£ (1 / (k + rank_position))
```

Where:
- `k` = constant (typically 60) to prevent over-emphasizing top positions
- `rank_position` = position of document in each retrieval result
- The sum is calculated across all queries that retrieved this document

#### ğŸ“Š RRF with k=60 Example

![RRF k=60 Example](images/rrf_k60_example.png)

**Why k=60?**

The constant `k=60` provides balance in the scoring:
- Higher positions still matter: `1/(60+1) = 0.0164`
- Lower positions get fair consideration: `1/(60+5) = 0.0154`
- Documents appearing in multiple queries get significantly higher scores through accumulation

**Example Calculation:**

If "Document X" appears in 2 different query results:
- Query 1, Position 1: `1/(60+1) = 0.0164`
- Query 2, Position 2: `1/(60+2) = 0.0161`
- **Total RRF Score: 0.0325** â† Much higher than single-query documents!

#### ğŸ’¡ Key Takeaways

![RRF Key Takeaways](images/rrf_key_takeaways.png)

**Benefits of RRF:**

1. âœ… **Consensus Effect** - Documents appearing in multiple query results get boosted
2. âœ… **Balanced Scoring** - k=60 prevents over-emphasis on top positions while maintaining order
3. âœ… **Diversity Preservation** - Unique chunks from each query still contribute to final ranking
4. âœ… **Simple but Effective** - Despite simplicity, RRF often outperforms complex fusion methods
5. âœ… **No Training Required** - Works immediately without parameter tuning or model training

#### ğŸš€ Implementation

```python
from collections import defaultdict

def reciprocal_rank_fusion(chunk_list, k=60, verbose=True):
    """
    Calculate RRF scores for documents retrieved from multiple queries.
    
    Args:
        chunk_list: List of retrieval results (each is a list of document chunks)
        k: RRF constant (default: 60)
        verbose: Print detailed scoring information
    
    Returns:
        List of document chunks sorted by RRF score (highest first)
    """
    rrf_scores = defaultdict(float)
    all_unique_chunks = {}
    
    # Calculate RRF scores across all queries
    for query_idx, chunks in enumerate(chunk_list, 1):
        for position, chunk in enumerate(chunks, 1):
            chunk_content = chunk.page_content
            
            # Store chunk object
            all_unique_chunks[chunk_content] = chunk
            
            # Accumulate RRF score: 1/(k + position)
            position_score = 1 / (k + position)
            rrf_scores[chunk_content] += position_score
    
    # Sort by RRF score (highest first)
    sorted_chunks = sorted(
        all_unique_chunks.items(),
        key=lambda x: rrf_scores[x[0]],
        reverse=True
    )
    
    return [chunk_obj for chunk_content, chunk_obj in sorted_chunks]

# Example usage with multi-query retrieval
all_retrieval_results = []
for query in query_variations:
    docs = retriever.invoke(query)
    all_retrieval_results.append(docs)

# Fuse results using RRF
fused_results = reciprocal_rank_fusion(all_retrieval_results, k=60)
```

#### ğŸ¯ Best For:

- âœ… **Multi-query retrieval** - Combining results from query variations
- âœ… **Cross-lingual search** - Merging results from different language queries
- âœ… **Multi-modal retrieval** - Fusing text, image, and other retrieval results
- âœ… **Ensemble methods** - Combining outputs from different retrieval algorithms
- âœ… **Improving recall** - Ensuring diverse, relevant documents aren't missed

#### ğŸ”¬ When to Use RRF vs Other Methods

| Method | Best Use Case | Strength |
|--------|--------------|----------|
| **Basic Similarity** | Single simple query | Fast, straightforward |
| **Score Threshold** | High precision needs | Quality filtering |
| **MMR** | Avoiding redundancy | Diversity within single query |
| **Multi-Query** | Complex questions | Multiple perspectives |
| **RRF** | Combining multiple retrievals | Consensus + diversity |

**Pro Tip:** Combine Multi-Query Retrieval + RRF for best results on complex questions!

---

### Method 6: Hybrid Search (Vector + BM25)

**File:** `hybrid_search.ipynb`

Hybrid search combines the strengths of semantic search (vector embeddings) and keyword search (BM25) to achieve superior retrieval accuracy. This ensemble approach leverages the complementary nature of dense and sparse retrieval methods.

#### ğŸ¯ How Hybrid Search Works

![Hybrid Search Flow](images/hybrid_search_flow.png)

**The Two Retrieval Methods:**

1. **Vector Retriever (Dense/Semantic Search)**
   - Uses sentence embeddings to understand semantic meaning
   - Excellent for conceptual queries and paraphrased questions
   - Captures context and relationships between words
   - Model: `sentence-transformers/all-MiniLM-L6-v2`

2. **BM25 Retriever (Sparse/Keyword Search)**
   - Uses term frequency and inverse document frequency
   - Excellent for exact matches and specific terms
   - Captures precise keywords and product names
   - Based on probabilistic ranking function

**Ensemble Strategy:**

```python
from langchain_classic.retrievers.ensemble import EnsembleRetriever

hybrid_retriever = EnsembleRetriever(
    retrievers=[vector_retriever, bm25_retriever],
    weights=[0.5, 0.5]  # Equal weighting (adjustable)
)
```

#### ğŸ’¡ Key Benefits of Hybrid Search

| Aspect | Vector Only | BM25 Only | **Hybrid (Best of Both)** |
|--------|------------|-----------|---------------------------|
| **Semantic Understanding** | âœ… Excellent | âŒ Limited | âœ… Excellent |
| **Exact Term Matching** | âŒ Weak | âœ… Excellent | âœ… Excellent |
| **Handling Synonyms** | âœ… Good | âŒ Poor | âœ… Good |
| **Product Names/IDs** | âŒ May miss | âœ… Catches | âœ… Catches |
| **Misspellings** | âœ… Tolerant | âŒ Strict | âœ… Tolerant |
| **Overall Accuracy** | âš ï¸ Medium | âš ï¸ Medium | âœ… **High** |

#### ğŸ“Š Example Query Performance

**Query:** *"purchase cost 7.5 billion"*

- **Vector Search Strength:** Understands "purchase cost" semantically (acquisition price)
- **BM25 Search Strength:** Finds exact match for "7.5 billion"
- **Hybrid Result:** Correctly retrieves "Microsoft acquired GitHub for 7.5 billion dollars"

**Query:** *"electric vehicle manufacturing Cybertruck"*

- **Vector Search Strength:** Understands "electric vehicle manufacturing" concept
- **BM25 Search Strength:** Finds exact match for "Cybertruck" product name
- **Hybrid Result:** Returns all relevant Cybertruck production and EV context

#### ğŸš€ Implementation

```python
# 1. Setup Vector Retriever
embedding_model = HuggingFaceEmbeddings(
    model="sentence-transformers/all-MiniLM-L6-v2"
)
vectorstore = Chroma.from_documents(
    documents=documents,
    embedding=embedding_model
)
vector_retriever = vectorstore.as_retriever(search_kwargs={"k": 10})

# 2. Setup BM25 Retriever
bm25_retriever = BM25Retriever.from_documents(documents)
bm25_retriever.k = 10

# 3. Create Hybrid Retriever
hybrid_retriever = EnsembleRetriever(
    retrievers=[vector_retriever, bm25_retriever],
    weights=[0.5, 0.5]
)

# 4. Retrieve Documents
results = hybrid_retriever.invoke("your query here")
```

#### ğŸ¯ Best For:

- âœ… **Mixed queries** - Combining concepts and specific terms
- âœ… **Product search** - Finding specific items with context
- âœ… **Financial data** - Combining metrics with company names
- âœ… **Technical queries** - Balancing jargon and concepts
- âœ… **General purpose** - Works well across diverse query types

---

### Method 7: Reranking with Cross-Encoders

**File:** `reranker.ipynb`

Reranking is the critical second stage in a high-accuracy RAG pipeline. After retrieving candidate documents using hybrid search, a reranker model analyzes the query-document relationship more deeply to produce a precise final ranking.

#### ğŸ¤” Why Do We Need Reranking?

![Why Reranker is Needed](images/reranker_why_needed.png)

**Embeddings Limitation:**

While vector embeddings and hybrid search excel at casting a wide net to find potentially relevant documents, they have inherent limitations:

**Example Query:** *"How to fix a leaky faucet?"*

- **Chunk A:** "To repair a dripping tap, first turn off the water supply, then remove the handle and replace the worn washer."
  - **Embedding Score:** 0.82 (high similarity!)
- **Chunk B:** "Water damage from leaky faucets can cost homeowners thousands in repairs and lead to mold growth."
  - **Embedding Score:** 0.79 (also high similarity!)

**The Problem:** Both chunks mention "leaky faucets" and score highly, but only Chunk A actually answers the question. Embeddings alone can't distinguish between:
- âœ… **Direct answers** vs âŒ **Related but unhelpful content**

**The Solution:** Rerankers use cross-encoders that read the query and chunk *together* to understand their true relationship.

#### ğŸ¯ The Two-Stage Strategy

![Two-Stage Strategy Comparison](images/reranker_comparison.png)

**Stage 1: Embeddings (Fast & Broad)**

Purpose: Inexpensive approximation to find candidates

```
Query â†’ Vector â†’ Compare with 1M+ chunks â†’ Top 100 chunks
```

**Characteristics:**
- âš¡ **Fast** - Can search millions of chunks quickly
- ğŸŒ **Broad** - Good at finding chunks in the right neighborhood
- ğŸ’° **Cheap** - Low computational cost per comparison
- âœ… **Reliable** - Gives good probability of relevance

**BUT:** It's just an approximation. You wouldn't bet your money on the exact ranking.

**Stage 2: Reranker (Precise & Focused)**

![Reranker Two-Stage Strategy](images/reranker_two_stage_strategy.png)

Purpose: Increase probability that top 10 are the absolute best

```
Query + Each of 25 chunks â†’ Reranker/Cross-encoder â†’ Precise relevance scores
```

**Characteristics:**
- ğŸ¯ **Precise** - Actually reads query and chunk together
- ğŸ§  **Context-aware** - Understands query intent and relationships
- ğŸ’¸ **Expensive** - Higher computational cost, but only for 10-100 chunks usually

**Why This Two-Stage Approach Works:**

1. **Embeddings:** Cast a wide, inexpensive net to find chunk candidates
2. **Reranker:** Apply slightly more expensive but precise analysis to finalize ranking

*It's like having a screening interview followed by a detailed technical interview - each stage optimized for its purpose.*

#### ğŸ§  How Does Reranker (Cross-Encoder) Achieve This?

![Reranker Cross-Encoder Mechanism](images/reranker_cross_encoder.png)

**Reranker (Cross-encoder/Joint-processing approach):**

```
Combined Input: "apple stock price [SEP] Apple trees grow in orchards"
     â†“ (processed together)
Cross-encoder analyzes relationship
     â†“
Relevance Score: 0.12 (correctly identifies mismatch)
```

**Advantage:** The reranker model reads both the query and chunk simultaneously, understanding their relationship context.

#### ğŸš€ Implementation

```python
from langchain_cohere import CohereRerank

# Step 1: Get initial results from hybrid search
retrieved_docs = hybrid_retriever.invoke(query)  # Get top 25-50

# Step 2: Rerank using Cohere's cross-encoder model
reranker = CohereRerank(
    model="rerank-english-v3.0",
    top_n=10,  # Final number of documents to return
    cohere_api_key=os.getenv("COHERE_API_KEY")
)

reranked_docs = reranker.compress_documents(retrieved_docs, query)

# Step 3: Use top reranked documents for RAG
top_context = reranked_docs[:5]
```

#### ğŸ“Š Real-World Example

**Query:** *"Tesla financial performance and production updates"*

**Before Reranking (Hybrid Search Top 5):**
1. Tesla reported record quarterly revenue of $25.2 billion in Q3 2024.
2. Tesla announced plans to expand Gigafactory production capacity.
3. Tesla reported strong free cash flow generation of $7.5 billion.
4. Tesla stock price reached new highs following earnings announcement.
5. Tesla continues to lead in electric vehicle market share globally.

**After Reranking (Top 5):**
1. Tesla reported strong free cash flow generation of $7.5 billion. â¬†ï¸
2. Tesla reported record quarterly revenue of $25.2 billion in Q3 2024. âœ“
3. **Tesla's automotive gross margin improved to 19.3% this quarter.** â¬†ï¸â¬†ï¸ (more specific financial metric)
4. Tesla announced plans to expand Gigafactory production capacity. â¬†ï¸
5. **Tesla's energy storage business grew 40% year-over-year.** â¬†ï¸â¬†ï¸ (quantified performance metric)

**Key Improvements:**
- âœ… More specific financial metrics moved higher
- âœ… Quantified performance data prioritized
- âœ… Better alignment with "performance and production" intent

#### ğŸ¯ Best For:

- âœ… **High-accuracy RAG** - When precision is critical
- âœ… **Complex queries** - Multi-faceted questions needing nuanced understanding
- âœ… **Final ranking refinement** - After hybrid/multi-query retrieval
- âœ… **Reducing false positives** - Filtering semantically similar but irrelevant content
- âœ… **Production systems** - Where answer quality directly impacts user experience

#### ğŸ”¬ When to Use Reranking

| Scenario | Use Reranking? | Rationale |
|----------|---------------|-----------|
| **Simple FAQ lookup** | âŒ Optional | Hybrid search may suffice |
| **Customer-facing chatbot** | âœ… **Yes** | High accuracy needed |
| **Research/Analysis tool** | âœ… **Yes** | Precision critical |
| **Large candidate pool (100+)** | âœ… **Yes** | Ranking quality matters |
| **Cost-sensitive application** | âš ï¸ Consider | Balance cost vs accuracy |

#### ğŸ’° Cost-Benefit Analysis

**Reranking Costs:**
- API calls to Cohere (or self-hosted cross-encoder)
- Typically $0.001 - $0.005 per rerank request

**Benefits:**
- 15-30% improvement in answer accuracy
- Significantly better top-3 result quality
- Reduced hallucinations from LLM due to better context

**Recommendation:** For production RAG systems where quality matters, reranking ROI is almost always positive.

---

## ğŸ§ª Technologies Used

| Category | Technology |
|----------|-----------|
| **Framework** | ![LangChain](https://img.shields.io/badge/LangChain-Latest-green) |
| **Embeddings** | ![HuggingFace](https://img.shields.io/badge/HuggingFace-Transformers-yellow) |
| **Vector DB** | ![ChromaDB](https://img.shields.io/badge/ChromaDB-Vector%20Store-orange) |
| **LLM** | ![OpenAI](https://img.shields.io/badge/OpenAI-GPT--4-blue) |
| **LLM Router** | ![OpenRouter](https://img.shields.io/badge/OpenRouter-API-purple) |
| **Language** | ![Python](https://img.shields.io/badge/Python-3.11+-blue) |
| **Notebook** | ![Jupyter](https://img.shields.io/badge/Jupyter-Interactive-orange) |
| **Validation** | ![Pydantic](https://img.shields.io/badge/Pydantic-Structured%20Output-red) |

---

## ğŸ“ˆ Performance Metrics

- âš¡ **Embedding Speed:** ~100 chunks/second
- ğŸ’¾ **Storage Efficiency:** ~1KB per chunk (compressed)
- ğŸ” **Retrieval Latency:** <100ms for top-5 results
- ğŸ¯ **Accuracy:** 85%+ for domain-specific queries

---

## ğŸ¤ Contributing

Contributions are welcome! Please follow these steps:

1. **Fork the repository**
2. **Create a feature branch** (`git checkout -b feature/AmazingFeature`)
3. **Commit your changes** (`git commit -m 'Add some AmazingFeature'`)
4. **Push to the branch** (`git push origin feature/AmazingFeature`)
5. **Open a Pull Request**

---

## ğŸ“ License

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

---

## ğŸŒŸ Acknowledgments

- **LangChain** - For the amazing RAG framework
- **HuggingFace** - For open-source embedding models
- **ChromaDB** - For efficient vector storage
- **OpenAI** - For powerful language models

---

## ğŸ“ Contact

For questions or support, please open an issue or reach out:

- ğŸ“§ Email: kaustavdas2027@gmail.com
- ğŸ™ GitHub: [@kaustav3071](https://github.com/kaustav3071)
- ğŸ’¬ LinkedIn: [Kaustav Das](https://www.linkedin.com/in/kaustavdas1703/)

---

<div align="center">

**Made with â¤ï¸ by Kaustav Das**

â­ **Star this repo if you find it helpful!** â­

</div>
